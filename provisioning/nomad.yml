- hosts: localhost
  gather_facts: false

  vars:
    instance_cores: 1
    instance_cpuunits: 1000
    instance_node: pve
    instance_ostemplate: local:vztmpl/debian-11-with-ssh_11.6-1_amd64.tar.zst
    proxmox_host: pve.{{ local_domain }}:8006

  roles:
    - role: proxmox-instance
      action: create
      instance_hostname: nomad1
      instance_password: abcde
      instance_type: lxc
    - role: proxmox-instance
      action: create
      instance_hostname: nomad2
      instance_password: abcde
      instance_type: lxc
    - role: proxmox-instance
      action: create
      instance_hostname: nomad3
      instance_password: abcde
      instance_type: lxc

- name: Create admin users
  gather_facts: false
  hosts: nomad
  # We need to attempt a login with the admin user to check if these tasks need
  # to run. In order to do that, we need to be able to handle an unreachable
  # host.
  ignore_unreachable: true
  remote_user: ansible

  roles:
    - role: provisioning-user
      original_user: root
      original_password: abcde

- name: Install Nomad node prerequisites
  hosts: nomad
  remote_user: ansible

  roles:
    - software-update
    - ssh-harden
    - role: key-sync
      vars:
        key_sync_url: "{{ authorized_keys_url }}"
    - role: consul
      consul_mode: agent

- name: Provision Nomad servers
  hosts: nomad_servers
  remote_user: ansible

  roles:
    - role: nomad
      nomad_mode: server

- name: Provision Nomad client agents
  hosts: nomad_clients
  remote_user: ansible

  vars:
    nomad_volumes:
      - name: docker-registry
        path: /nfs/nomad/docker-registry
        read_only: false
      - name: home-assistant
        path: /nfs/nomad/home-assistant
        read_only: false
      - name: magic-mirror-config
        path: /nfs/nomad/magic-mirror/config
        read_only: false
      - name: magic-mirror-css
        path: /nfs/nomad/magic-mirror/css
        read_only: false
      - name: nzbget
        path: /opt/nzbget
        read_only: false
      - name: prowlarr
        path: /opt/prowlarr
        read_only: false
      - name: radarr
        path: /opt/radarr
        read_only: false
      - name: sonarr
        path: /opt/sonarr
        read_only: false
      - name: traefik-certs-production
        path: /nfs/nomad/traefik-certs/production
        read_only: false
      - name: traefik-certs-staging
        path: /nfs/nomad/traefik-certs/production
        read_only: false
      - name: zwave-js-store
        path: /nfs/nomad/zwave-js/store
        read_only: false

  pre_tasks:
    - name: Install NFS packages
      become: true
      ansible.builtin.package:
        name: nfs-common
        state: present

    - name: Mount NAS
      become: true
      ansible.posix.mount:
        boot: true
        fstype: nfs
        opts: auto,nofail,noatime,nolock,intr,tcp,actimeo=1800,x-systemd.automount
        path: /nfs/nomad
        src: freenas.lan.qidux.com:/mnt/Primary/nomad
        state: mounted

    - name: Mount media
      become: true
      ansible.posix.mount:
        boot: true
        fstype: nfs
        opts: auto,nofail,noatime,nolock,intr,tcp,actimeo=1800,x-systemd.automount
        path: /nfs/media
        src: freenas.lan.qidux.com:/mnt/Primary/Media
        state: mounted

    - name: Ensure Nomad volume directories exist
      become: true
      ansible.builtin.file:
        mode: "755"
        path: "{{ item.path }}"
        state: directory
      loop: "{{ nomad_volumes }}"

  roles:
    - docker
    - role: nomad
      nomad_mode: client
      nomad_client_meta:
        cluster: compute

- name: Provision Nomad clients for DNS
  hosts: dns
  remote_user: ansible

  roles:
    - role: nomad
      nomad_mode: client
      nomad_client_meta:
        cluster: dns
      # Need raw exec for tailscale job since the isolation primitives aren't
      # present on our Raspberry Pi machines.
      nomad_enable_raw_exec: true
